{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800748b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from reconstruct_coordinates import reconstruct_coords_batch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b4bbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "  X (Coulomb matrices): (7211, 23, 23)\n",
      "  T (Properties): (7211, 14)\n",
      "Reconstructing coordinate arrays from Coulomb matrices...\n",
      "Processing molecule 100/7211...\n",
      "Processing molecule 200/7211...\n",
      "Processing molecule 300/7211...\n",
      "Processing molecule 400/7211...\n",
      "Processing molecule 500/7211...\n",
      "Processing molecule 600/7211...\n",
      "Processing molecule 700/7211...\n",
      "Processing molecule 800/7211...\n",
      "Processing molecule 900/7211...\n",
      "Processing molecule 1000/7211...\n",
      "Processing molecule 1100/7211...\n",
      "Processing molecule 1200/7211...\n",
      "Processing molecule 1300/7211...\n",
      "Processing molecule 1400/7211...\n",
      "Processing molecule 1500/7211...\n",
      "Processing molecule 1600/7211...\n",
      "Processing molecule 1700/7211...\n",
      "Processing molecule 1800/7211...\n",
      "Processing molecule 1900/7211...\n",
      "Processing molecule 2000/7211...\n",
      "Processing molecule 2100/7211...\n",
      "Processing molecule 2200/7211...\n",
      "Processing molecule 2300/7211...\n",
      "Processing molecule 2400/7211...\n",
      "Processing molecule 2500/7211...\n",
      "Processing molecule 2600/7211...\n",
      "Processing molecule 2700/7211...\n",
      "Processing molecule 2800/7211...\n",
      "Processing molecule 2900/7211...\n",
      "Processing molecule 3000/7211...\n",
      "Processing molecule 3100/7211...\n",
      "Processing molecule 3200/7211...\n",
      "Processing molecule 3300/7211...\n",
      "Processing molecule 3400/7211...\n",
      "Processing molecule 3500/7211...\n",
      "Processing molecule 3600/7211...\n",
      "Processing molecule 3700/7211...\n",
      "Processing molecule 3800/7211...\n",
      "Processing molecule 3900/7211...\n",
      "Processing molecule 4000/7211...\n",
      "Processing molecule 4100/7211...\n",
      "Processing molecule 4200/7211...\n",
      "Processing molecule 4300/7211...\n",
      "Processing molecule 4400/7211...\n",
      "Processing molecule 4500/7211...\n",
      "Processing molecule 4600/7211...\n",
      "Processing molecule 4700/7211...\n",
      "Processing molecule 4800/7211...\n",
      "Processing molecule 4900/7211...\n",
      "Processing molecule 5000/7211...\n",
      "Processing molecule 5100/7211...\n",
      "Processing molecule 5200/7211...\n",
      "Processing molecule 5300/7211...\n",
      "Processing molecule 5400/7211...\n",
      "Processing molecule 5500/7211...\n",
      "Processing molecule 5600/7211...\n",
      "Processing molecule 5700/7211...\n",
      "Processing molecule 5800/7211...\n",
      "Processing molecule 5900/7211...\n",
      "Processing molecule 6000/7211...\n",
      "Processing molecule 6100/7211...\n",
      "Processing molecule 6200/7211...\n",
      "Processing molecule 6300/7211...\n",
      "Processing molecule 6400/7211...\n",
      "Processing molecule 6500/7211...\n",
      "Processing molecule 6600/7211...\n",
      "Processing molecule 6700/7211...\n",
      "Processing molecule 6800/7211...\n",
      "Processing molecule 6900/7211...\n",
      "Processing molecule 7000/7211...\n",
      "Processing molecule 7100/7211...\n",
      "Processing molecule 7200/7211...\n",
      "Extracted Z for 7211 molecules\n"
     ]
    }
   ],
   "source": [
    "data = scipy.io.loadmat('qm7b.mat')\n",
    "\n",
    "# Extract available data\n",
    "X = data['X']  # Coulomb matrices: (7211, 23, 23)\n",
    "T = data['T']  # Properties: (7211, 14)\n",
    "names = data['names']  # Property names: (14,)\n",
    "\n",
    "X = np.array(X)\n",
    "T = np.array(T)\n",
    "\n",
    "# Get property names\n",
    "if names.ndim > 1:\n",
    "    property_names = [str(names[i][0]) for i in range(len(names))]\n",
    "else:\n",
    "    property_names = [str(names[i]) for i in range(len(names))]\n",
    "\n",
    "print(f\"Dataset shapes:\")\n",
    "print(f\"  X (Coulomb matrices): {X.shape}\")\n",
    "print(f\"  T (Properties): {T.shape}\")\n",
    "\n",
    "# Extract atomic numbers from Coulomb matrix diagonals\n",
    "# C_ii = 0.5 * Z_i^2.4, so Z_i = (2 * C_ii)^(1/2.4)\n",
    "Z_list = []\n",
    "for i in range(len(X)):\n",
    "    coulomb_diag = np.diag(X[i])\n",
    "    # Find non-zero diagonal elements (actual atoms)\n",
    "    atom_indices = np.where(coulomb_diag > 1e-6)[0]\n",
    "    if len(atom_indices) > 0:\n",
    "        z_vals = (2 * coulomb_diag[atom_indices]) ** (1.0 / 2.4)\n",
    "        z_vals = np.round(z_vals).astype(int)\n",
    "        Z_list.append(z_vals)\n",
    "    else:\n",
    "        Z_list.append(np.array([1]))\n",
    "\n",
    "\n",
    "print(\"Reconstructing coordinate arrays from Coulomb matrices...\")\n",
    "R_list = reconstruct_coords_batch(X, Z_list, method='mds', verbose=True)\n",
    "\n",
    "Z = np.array(Z_list, dtype=object)\n",
    "R = np.array(R_list, dtype=object)\n",
    "\n",
    "print(f\"Extracted Z for {len(Z)} molecules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7161383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spherical_harmonics(r, max_l=2):\n",
    "    batch_size, n_atoms, _ = r.shape\n",
    "    device = r.device\n",
    "    \n",
    "    # Convert into spherical coordinates\n",
    "    x, y, z = r[..., 0], r[..., 1], r[..., 2]\n",
    "    r_norm = torch.norm(r, dim=-1, keepdim=True) \n",
    "\n",
    "    r_norm = r_norm + 1e-8 # avoid division by zero\n",
    "    cos_theta = z / r_norm.squeeze(-1)\n",
    "    theta = torch.acos(torch.clamp(cos_theta, -1.0, 1.0))\n",
    "    phi = torch.atan2(y, x)\n",
    "    \n",
    "    harmonics = []\n",
    "    \n",
    "    for l in range(max_l + 1):\n",
    "        for m in range(-l, l + 1):\n",
    "            if l == 0:\n",
    "                # Y_0^0 = 1/sqrt(4*pi) - constant term (s orbital)\n",
    "                y_lm = torch.ones(batch_size, n_atoms, device=device) / np.sqrt(4 * np.pi)\n",
    "            elif l == 1:\n",
    "                if m == -1:\n",
    "                    # Y_1^{-1}\n",
    "                    y_lm = np.sqrt(3 / (4 * np.pi)) * torch.sin(theta) * torch.cos(phi)\n",
    "                elif m == 0:\n",
    "                    # Y_1^0\n",
    "                    y_lm = np.sqrt(3 / (4 * np.pi)) * cos_theta\n",
    "                elif m == 1:\n",
    "                    # Y_1^1\n",
    "                    y_lm = -np.sqrt(3 / (4 * np.pi)) * torch.sin(theta) * torch.sin(phi)\n",
    "            elif l == 2:\n",
    "                if m == -2:\n",
    "                    # Y_2^{-2}\n",
    "                    y_lm = np.sqrt(15 / (4 * np.pi)) * torch.sin(theta) ** 2 * torch.sin(2 * phi) / 2\n",
    "                elif m == -1:\n",
    "                    # Y_2^{-1}\n",
    "                    y_lm = np.sqrt(15 / (4 * np.pi)) * torch.sin(theta) * cos_theta * torch.sin(phi)\n",
    "                elif m == 0:\n",
    "                    # Y_2^0\n",
    "                    y_lm = np.sqrt(5 / (4 * np.pi)) * (3 * cos_theta ** 2 - 1) / 2\n",
    "                elif m == 1:\n",
    "                    # Y_2^1\n",
    "                    y_lm = -np.sqrt(15 / (4 * np.pi)) * torch.sin(theta) * cos_theta * torch.cos(phi)\n",
    "                elif m == 2:\n",
    "                    # Y_2^2\n",
    "                    y_lm = np.sqrt(15 / (4 * np.pi)) * torch.sin(theta) ** 2 * torch.cos(2 * phi) / 2\n",
    "            else:\n",
    "                y_lm = torch.zeros(batch_size, n_atoms, device=device)\n",
    "            \n",
    "            harmonics.append(y_lm)\n",
    "    \n",
    "    harmonics_tensor = torch.stack(harmonics, dim=-1)\n",
    "    return harmonics_tensor\n",
    "\n",
    "\n",
    "def compute_radial_basis(r, n_radial=8, cutoff=5.0):\n",
    "    r_norm = torch.norm(r, dim=-1, keepdim=True)  # (batch, n_atoms, 1)\n",
    "    \n",
    "    centers = torch.linspace(0.0, cutoff, n_radial, device=r.device)\n",
    "    widths = cutoff / n_radial\n",
    "    \n",
    "    r_expanded = r_norm.squeeze(-1).unsqueeze(-1)\n",
    "    centers_expanded = centers.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    radial_basis = torch.exp(-((r_expanded - centers_expanded) / widths) ** 2)\n",
    "    \n",
    "    return radial_basis\n",
    "\n",
    "\n",
    "class SphericalConv(nn.Module):\n",
    "    def __init__(self, n_in, n_out, max_l=2, n_radial=8, cutoff=5.0):\n",
    "        super(SphericalConv, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.max_l = max_l\n",
    "        self.n_radial = n_radial\n",
    "        self.cutoff = cutoff\n",
    "        \n",
    "        # num of spherical harmonics: (max_l+1)^2, e.g. max_l=2, 9 harmonics\n",
    "        self.n_harmonics = (max_l + 1) ** 2\n",
    "        \n",
    "        # spherical feature dimension: n_harmonics * n_radial\n",
    "        self.spherical_dim = self.n_harmonics * n_radial\n",
    "        \n",
    "        # project to spherical space\n",
    "        self.to_spherical = nn.Linear(n_in, self.spherical_dim)\n",
    "        self.spherical_conv = nn.Linear(self.spherical_dim, self.spherical_dim)\n",
    "        self.from_spherical = nn.Linear(self.spherical_dim, n_out)\n",
    "        self.norm = nn.LayerNorm(n_out)\n",
    "        \n",
    "    def forward(self, x, positions, mask):\n",
    "        batch_size, n_atoms, _ = x.shape\n",
    "        \n",
    "        harmonics = compute_spherical_harmonics(positions, max_l=self.max_l)\n",
    "        radial = compute_radial_basis(positions, n_radial=self.n_radial, cutoff=self.cutoff)\n",
    "        \n",
    "        harmonics_expanded = harmonics.unsqueeze(-1)\n",
    "        radial_expanded = radial.unsqueeze(-2)\n",
    "        spherical_features = (harmonics_expanded * radial_expanded).view(batch_size, n_atoms, -1)\n",
    "        \n",
    "        x_spherical = self.to_spherical(x)\n",
    "        x_spherical = x_spherical * spherical_features\n",
    "        x_spherical = self.spherical_conv(x_spherical)\n",
    "        x_spherical = F.relu(x_spherical)\n",
    "        \n",
    "        out = self.from_spherical(x_spherical)\n",
    "        out = self.norm(out)\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class SphericalMessagePassing(nn.Module):\n",
    "    def __init__(self, n_in, n_out, max_l=2, n_radial=8, cutoff=5.0, eps=0.0, train_eps=True):\n",
    "        super(SphericalMessagePassing, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.initial_eps = eps\n",
    "        \n",
    "        if train_eps:\n",
    "            self.eps = nn.Parameter(torch.tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.tensor([eps]))\n",
    "        \n",
    "        self.spherical_conv = SphericalConv(n_in, n_out, max_l, n_radial, cutoff)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_out, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_out, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, positions, edge_index, mask):\n",
    "        batch_size = positions.shape[0]\n",
    "        n_atoms = positions.shape[1]\n",
    "        \n",
    "        x_reshaped = x.view(batch_size, n_atoms, -1)\n",
    "        mask_reshaped = mask.view(batch_size, n_atoms)\n",
    "        x_spherical = self.spherical_conv(x_reshaped, positions, mask_reshaped)\n",
    "        x_flat = x_spherical.view(-1, self.n_out)\n",
    "        row, col = edge_index\n",
    "        neighbor_features = x_flat[col]\n",
    "        \n",
    "        out = torch.zeros_like(x_flat)\n",
    "        if len(row) > 0:\n",
    "            out.index_add_(0, row, neighbor_features)\n",
    "        \n",
    "        out = (1 + self.eps) * x_flat + out\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "        \n",
    "        out_flat = out.view(-1, self.n_out)\n",
    "        out_mlp = self.mlp(out_flat)\n",
    "        out = out_mlp.view(batch_size * n_atoms, -1)\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class GSCNNLayer(nn.Module):\n",
    "    def __init__(self, n_in, n_out, max_l=2, n_radial=8, cutoff=5.0, eps=0.0, train_eps=True):\n",
    "        super(GSCNNLayer, self).__init__()\n",
    "        self.spherical_mp = SphericalMessagePassing(n_in, n_out, max_l, n_radial, cutoff, eps, train_eps)\n",
    "        self.norm = nn.LayerNorm(n_out)\n",
    "        \n",
    "        if n_in != n_out:\n",
    "            self.residual_proj = nn.Linear(n_in, n_out)\n",
    "        else:\n",
    "            self.residual_proj = None\n",
    "        \n",
    "    def forward(self, x, positions, edge_index, mask):\n",
    "        residual = x\n",
    "        if self.residual_proj is not None:\n",
    "            residual = self.residual_proj(residual)\n",
    "        \n",
    "        out = self.spherical_mp(x, positions, edge_index, mask)\n",
    "        out = self.norm(out)\n",
    "        out = out + residual\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class GSCNN(nn.Module):\n",
    "    def __init__(self, n_atom_basis=128, n_layers=3, \n",
    "                 n_out=14, max_atoms=23, dropout=0.1,\n",
    "                 max_l=2, n_radial=8, cutoff=5.0):\n",
    "        super(GSCNN, self).__init__()\n",
    "        self.n_atom_basis = n_atom_basis\n",
    "        self.n_layers = n_layers\n",
    "        self.max_atoms = max_atoms\n",
    "        self.dropout = dropout\n",
    "        self.max_l = max_l\n",
    "        self.n_radial = n_radial\n",
    "        self.cutoff = cutoff\n",
    "        \n",
    "        self.embedding = nn.Embedding(100, n_atom_basis)\n",
    "        self.gscnn_layers = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            self.gscnn_layers.append(\n",
    "                GSCNNLayer(n_atom_basis, n_atom_basis, max_l, n_radial, cutoff, eps=0.0, train_eps=True)\n",
    "            )\n",
    "        \n",
    "        self.atom_wise = nn.Sequential(\n",
    "            nn.Linear(n_atom_basis, n_atom_basis),\n",
    "            nn.BatchNorm1d(n_atom_basis),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_atom_basis, n_atom_basis),\n",
    "            nn.BatchNorm1d(n_atom_basis),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_atom_basis, n_atom_basis)\n",
    "        )\n",
    "        \n",
    "        self.property_head = nn.Sequential(\n",
    "            nn.Linear(n_atom_basis, n_atom_basis // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_atom_basis // 2, n_out)\n",
    "        )\n",
    "        \n",
    "    def build_edge_index(self, positions, mask, cutoff=5.0):\n",
    "        batch_size, n_atoms, _ = positions.shape\n",
    "        device = positions.device\n",
    "        \n",
    "        pos_i = positions.unsqueeze(2)  # (batch, n_atoms, 1, 3)\n",
    "        pos_j = positions.unsqueeze(1)  # (batch, 1, n_atoms, 3)\n",
    "        diff = pos_i - pos_j  # (batch, n_atoms, n_atoms, 3)\n",
    "        distances = torch.norm(diff, dim=-1)  # (batch, n_atoms, n_atoms)\n",
    "        \n",
    "        mask_i = mask.unsqueeze(2)  # (batch, n_atoms, 1)\n",
    "        mask_j = mask.unsqueeze(1)  # (batch, 1, n_atoms)\n",
    "        neighbor_mask = (mask_i * mask_j).float()  # (batch, n_atoms, n_atoms)\n",
    "        \n",
    "        eye = torch.eye(n_atoms, device=device).unsqueeze(0)\n",
    "        neighbor_mask = neighbor_mask * (1 - eye)\n",
    "        edge_mask = (distances < cutoff) * neighbor_mask\n",
    "        \n",
    "        edge_indices = []\n",
    "        for b in range(batch_size):\n",
    "            edges = torch.nonzero(edge_mask[b], as_tuple=False)\n",
    "            if len(edges) > 0:\n",
    "                edges_offset = edges + b * n_atoms\n",
    "                edge_indices.append(edges_offset.t())\n",
    "            else:\n",
    "                self_loop = torch.arange(n_atoms, device=device) + b * n_atoms\n",
    "                edge_indices.append(torch.stack([self_loop, self_loop]))\n",
    "        \n",
    "        if len(edge_indices) > 0:\n",
    "            edge_index = torch.cat(edge_indices, dim=1)\n",
    "        else:\n",
    "            edge_index = torch.stack([\n",
    "                torch.arange(batch_size * n_atoms, device=device),\n",
    "                torch.arange(batch_size * n_atoms, device=device)\n",
    "            ])\n",
    "        \n",
    "        return edge_index\n",
    "    \n",
    "    def forward(self, atomic_numbers, positions, mask):\n",
    "        batch_size, n_atoms, _ = positions.shape\n",
    "        \n",
    "        x = self.embedding(atomic_numbers)\n",
    "        edge_index = self.build_edge_index(positions, mask, cutoff=self.cutoff)\n",
    "        \n",
    "        x_flat = x.view(-1, self.n_atom_basis)\n",
    "        mask_flat = mask.view(-1)\n",
    "        \n",
    "        for gscnn_layer in self.gscnn_layers:\n",
    "            x_flat = gscnn_layer(x_flat, positions, edge_index, mask_flat)\n",
    "        \n",
    "        x = x_flat.view(batch_size, n_atoms, self.n_atom_basis)\n",
    "        \n",
    "        x_flat = x.view(-1, self.n_atom_basis)\n",
    "        x_flat = self.atom_wise(x_flat)\n",
    "        x = x_flat.view(batch_size, n_atoms, self.n_atom_basis)\n",
    "        \n",
    "        mask_expanded = mask.unsqueeze(-1).float()\n",
    "        x_masked = x * mask_expanded\n",
    "        x_sum = x_masked.sum(dim=1)\n",
    "        n_atoms = mask.sum(dim=1, keepdim=True).float()\n",
    "        x_mean = x_sum / (n_atoms + 1e-8)\n",
    "        \n",
    "        properties = self.property_head(x_mean)\n",
    "        \n",
    "        return properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c1128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QM7bDataset(Dataset):\n",
    "    def __init__(self, Z, R, T, max_atoms=23):\n",
    "        self.Z = Z \n",
    "        self.R = R \n",
    "        self.T = T \n",
    "        self.n_molecules = len(Z) \n",
    "        self.max_atoms = max_atoms \n",
    "        self.n_properties = T.shape[1] if T.ndim > 1 else 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_molecules\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        z = torch.from_numpy(self.Z[idx]).long() \n",
    "        r = torch.from_numpy(self.R[idx]).float() \n",
    "        t = torch.from_numpy(self.T[idx]).float() if self.T.ndim > 1 else torch.tensor([self.T[idx]]).float() \n",
    "        \n",
    "        mask = (z > 0).float()\n",
    "        \n",
    "        return z, r, mask, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4907bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_molecular_data(Z, R, max_atoms=23):\n",
    "    n_molecules = len(Z)\n",
    "    \n",
    "    Z_processed = []\n",
    "    R_processed = []\n",
    "    \n",
    "    for i in range(n_molecules):\n",
    "        z_vals = Z[i] if isinstance(Z[i], np.ndarray) else np.array([Z[i]])\n",
    "        z_vals = z_vals.flatten()\n",
    "        \n",
    "        r_vals = R[i] if isinstance(R[i], np.ndarray) else np.zeros((len(z_vals), 3))\n",
    "        if r_vals.ndim == 1:\n",
    "            if len(r_vals) >= len(z_vals) * 3:\n",
    "                r_vals = r_vals[:len(z_vals)*3].reshape(-1, 3)\n",
    "            else:\n",
    "                r_vals = np.zeros((len(z_vals), 3))\n",
    "        elif r_vals.ndim == 2 and r_vals.shape[1] != 3:\n",
    "            r_vals = np.zeros((len(z_vals), 3))\n",
    "        \n",
    "        if r_vals.shape[0] < len(z_vals):\n",
    "            r_padded_temp = np.zeros((len(z_vals), 3))\n",
    "            r_padded_temp[:r_vals.shape[0]] = r_vals\n",
    "            r_vals = r_padded_temp\n",
    "        elif r_vals.shape[0] > len(z_vals):\n",
    "            r_vals = r_vals[:len(z_vals)]\n",
    "        \n",
    "        n_atoms = min(len(z_vals), max_atoms)\n",
    "        z_padded = np.zeros(max_atoms, dtype=np.int64)\n",
    "        r_padded = np.zeros((max_atoms, 3), dtype=np.float32)\n",
    "        \n",
    "        z_padded[:n_atoms] = z_vals[:n_atoms]\n",
    "        r_padded[:n_atoms] = r_vals[:n_atoms]\n",
    "        \n",
    "        Z_processed.append(z_padded)\n",
    "        R_processed.append(r_padded)\n",
    "    \n",
    "    return np.array(Z_processed), np.array(R_processed)\n",
    "\n",
    "Z_processed, R_processed = prepare_molecular_data(Z, R, max_atoms=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3a7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5768\n",
      "Validation samples: 721\n",
      "Test samples: 722\n"
     ]
    }
   ],
   "source": [
    "# Split data to 80% train, 10% validation, 10% test\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(T)), test_size=0.2, random_state=42\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QM7bDataset(\n",
    "    Z_processed[train_idx], \n",
    "    R_processed[train_idx], \n",
    "    T[train_idx],\n",
    "    max_atoms=23\n",
    ")\n",
    "val_dataset = QM7bDataset(\n",
    "    Z_processed[val_idx], \n",
    "    R_processed[val_idx], \n",
    "    T[val_idx],\n",
    "    max_atoms=23\n",
    ")\n",
    "test_dataset = QM7bDataset(\n",
    "    Z_processed[test_idx], \n",
    "    R_processed[test_idx], \n",
    "    T[test_idx],\n",
    "    max_atoms=23\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591b42e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 245,825\n"
     ]
    }
   ],
   "source": [
    "model = GSCNN(\n",
    "    n_atom_basis=128,\n",
    "    n_layers=3,\n",
    "    n_out=T.shape[1],\n",
    "    max_atoms=23,\n",
    "    dropout=0.1,\n",
    "    max_l=2,\n",
    "    n_radial=8,\n",
    "    cutoff=5.0\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd967f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training - GSCNN\n",
      "Epoch [1/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  106492.748076  |  Train MAE:  85.409835\n",
      "    Val MSE:    1671.746704  |  Val MAE:    10.990288\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [2/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  2063.258740  |  Train MAE:  13.797783\n",
      "    Val MSE:    814.611977  |  Val MAE:    7.097909\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [3/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1667.997099  |  Train MAE:  9.998275\n",
      "    Val MSE:    479.932928  |  Val MAE:    5.793611\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [4/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1626.767938  |  Train MAE:  9.838125\n",
      "    Val MSE:    428.684047  |  Val MAE:    5.237374\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [5/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1480.151961  |  Train MAE:  9.861202\n",
      "    Val MSE:    416.720241  |  Val MAE:    6.119227\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [6/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1421.754212  |  Train MAE:  9.338892\n",
      "    Val MSE:    350.009509  |  Val MAE:    5.224762\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [7/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1384.217680  |  Train MAE:  9.888548\n",
      "    Val MSE:    408.424846  |  Val MAE:    5.920331\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [8/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1318.345834  |  Train MAE:  9.072571\n",
      "    Val MSE:    401.569779  |  Val MAE:    5.350812\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [9/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1235.113789  |  Train MAE:  8.956736\n",
      "    Val MSE:    315.960300  |  Val MAE:    5.611096\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [10/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1232.957650  |  Train MAE:  8.664157\n",
      "    Val MSE:    437.414534  |  Val MAE:    6.006970\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [11/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1167.674270  |  Train MAE:  8.652767\n",
      "    Val MSE:    273.533578  |  Val MAE:    4.825823\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [12/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1096.434276  |  Train MAE:  8.195059\n",
      "    Val MSE:    929.958395  |  Val MAE:    8.224721\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [13/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1086.474830  |  Train MAE:  8.382947\n",
      "    Val MSE:    888.213841  |  Val MAE:    8.575675\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [14/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1096.897614  |  Train MAE:  8.208604\n",
      "    Val MSE:    262.486763  |  Val MAE:    4.413375\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [15/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1055.967554  |  Train MAE:  7.978751\n",
      "    Val MSE:    404.961956  |  Val MAE:    5.657465\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [16/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1137.563230  |  Train MAE:  8.302318\n",
      "    Val MSE:    352.497561  |  Val MAE:    5.112999\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [17/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1070.629035  |  Train MAE:  8.215166\n",
      "    Val MSE:    270.893202  |  Val MAE:    5.381647\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [18/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1037.231823  |  Train MAE:  8.035442\n",
      "    Val MSE:    199.884697  |  Val MAE:    3.974578\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [19/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1044.947789  |  Train MAE:  8.031450\n",
      "    Val MSE:    423.504972  |  Val MAE:    5.778265\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [20/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1014.452200  |  Train MAE:  8.135573\n",
      "    Val MSE:    262.472464  |  Val MAE:    5.319819\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [21/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1032.132762  |  Train MAE:  8.050329\n",
      "    Val MSE:    269.384305  |  Val MAE:    4.647077\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [22/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1018.883275  |  Train MAE:  7.853183\n",
      "    Val MSE:    165.208536  |  Val MAE:    3.749274\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [23/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1028.516658  |  Train MAE:  8.097251\n",
      "    Val MSE:    177.444005  |  Val MAE:    4.208347\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [24/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1020.776863  |  Train MAE:  7.935701\n",
      "    Val MSE:    228.033715  |  Val MAE:    4.161246\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [25/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  1028.588746  |  Train MAE:  8.176772\n",
      "    Val MSE:    419.322089  |  Val MAE:    5.646373\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [26/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  998.691484  |  Train MAE:  8.189062\n",
      "    Val MSE:    231.497306  |  Val MAE:    4.426112\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [27/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  972.405030  |  Train MAE:  7.856010\n",
      "    Val MSE:    248.828079  |  Val MAE:    4.893066\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [28/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  992.155426  |  Train MAE:  7.920156\n",
      "    Val MSE:    185.496427  |  Val MAE:    4.094759\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [29/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  991.393626  |  Train MAE:  7.912550\n",
      "    Val MSE:    183.130609  |  Val MAE:    3.811480\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [30/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  996.281574  |  Train MAE:  7.795865\n",
      "    Val MSE:    315.815708  |  Val MAE:    5.046373\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [31/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  975.405641  |  Train MAE:  7.885963\n",
      "    Val MSE:    197.444754  |  Val MAE:    4.203789\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [32/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  999.334836  |  Train MAE:  7.808122\n",
      "    Val MSE:    223.701015  |  Val MAE:    4.260169\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [33/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  952.946250  |  Train MAE:  7.683283\n",
      "    Val MSE:    270.815055  |  Val MAE:    4.503138\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [34/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  959.208994  |  Train MAE:  7.626180\n",
      "    Val MSE:    157.241084  |  Val MAE:    3.639914\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [35/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  935.076570  |  Train MAE:  7.522600\n",
      "    Val MSE:    146.356064  |  Val MAE:    3.574232\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [36/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  908.825056  |  Train MAE:  7.466256\n",
      "    Val MSE:    202.669051  |  Val MAE:    4.093941\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [37/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  908.744955  |  Train MAE:  7.478189\n",
      "    Val MSE:    167.794395  |  Val MAE:    3.764978\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [38/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  920.269052  |  Train MAE:  7.499900\n",
      "    Val MSE:    147.026060  |  Val MAE:    3.665996\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [39/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  914.179182  |  Train MAE:  7.441504\n",
      "    Val MSE:    225.432163  |  Val MAE:    4.302926\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [40/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  918.010263  |  Train MAE:  7.523295\n",
      "    Val MSE:    127.882409  |  Val MAE:    3.663965\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [41/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  916.312894  |  Train MAE:  7.509247\n",
      "    Val MSE:    242.945206  |  Val MAE:    4.359914\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [42/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  900.390669  |  Train MAE:  7.456602\n",
      "    Val MSE:    141.225819  |  Val MAE:    3.765729\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [43/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  913.675042  |  Train MAE:  7.592248\n",
      "    Val MSE:    149.453969  |  Val MAE:    3.553890\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [44/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  962.844882  |  Train MAE:  7.665141\n",
      "    Val MSE:    150.577244  |  Val MAE:    3.613524\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [45/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  889.952296  |  Train MAE:  7.311785\n",
      "    Val MSE:    204.641068  |  Val MAE:    4.153941\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [46/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  898.933057  |  Train MAE:  7.395478\n",
      "    Val MSE:    255.090537  |  Val MAE:    4.892950\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [47/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  888.492087  |  Train MAE:  7.375025\n",
      "    Val MSE:    181.013636  |  Val MAE:    3.835609\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [48/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  882.846134  |  Train MAE:  7.361427\n",
      "    Val MSE:    136.785358  |  Val MAE:    3.744254\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [49/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  903.483566  |  Train MAE:  7.369771\n",
      "    Val MSE:    117.258131  |  Val MAE:    3.163068\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [50/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  905.944179  |  Train MAE:  7.379940\n",
      "    Val MSE:    128.908800  |  Val MAE:    3.506908\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [51/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  880.757151  |  Train MAE:  7.317942\n",
      "    Val MSE:    130.532749  |  Val MAE:    3.401563\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [52/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  896.158813  |  Train MAE:  7.417141\n",
      "    Val MSE:    142.689862  |  Val MAE:    3.480411\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [53/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  922.443086  |  Train MAE:  7.509910\n",
      "    Val MSE:    173.696955  |  Val MAE:    4.075017\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [54/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  892.538309  |  Train MAE:  7.404242\n",
      "    Val MSE:    243.545582  |  Val MAE:    4.529366\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [55/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  891.505497  |  Train MAE:  7.506729\n",
      "    Val MSE:    163.426337  |  Val MAE:    3.829473\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [56/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  928.765658  |  Train MAE:  7.491396\n",
      "    Val MSE:    110.546934  |  Val MAE:    3.364062\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [57/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  884.493383  |  Train MAE:  7.378633\n",
      "    Val MSE:    124.445762  |  Val MAE:    3.261060\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [58/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  876.390276  |  Train MAE:  7.318382\n",
      "    Val MSE:    126.201003  |  Val MAE:    3.462496\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [59/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  949.512066  |  Train MAE:  7.531723\n",
      "    Val MSE:    173.715270  |  Val MAE:    3.732418\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [60/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  905.652205  |  Train MAE:  7.356929\n",
      "    Val MSE:    243.742779  |  Val MAE:    4.597298\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [61/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  912.365109  |  Train MAE:  7.434235\n",
      "    Val MSE:    110.088956  |  Val MAE:    3.152685\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [62/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  901.738668  |  Train MAE:  7.505291\n",
      "    Val MSE:    204.967627  |  Val MAE:    4.174916\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [63/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  920.164581  |  Train MAE:  7.471129\n",
      "    Val MSE:    158.408922  |  Val MAE:    3.657301\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [64/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  921.403091  |  Train MAE:  7.474058\n",
      "    Val MSE:    131.862902  |  Val MAE:    3.342246\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [65/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  858.316614  |  Train MAE:  7.230133\n",
      "    Val MSE:    118.026867  |  Val MAE:    3.237773\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [66/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  921.516079  |  Train MAE:  7.507275\n",
      "    Val MSE:    134.873422  |  Val MAE:    3.728684\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [67/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  894.199573  |  Train MAE:  7.378648\n",
      "    Val MSE:    135.713460  |  Val MAE:    3.315004\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [68/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  911.075833  |  Train MAE:  7.421970\n",
      "    Val MSE:    149.451942  |  Val MAE:    3.569781\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [69/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  911.991438  |  Train MAE:  7.455915\n",
      "    Val MSE:    118.954998  |  Val MAE:    3.215516\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [70/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  874.862504  |  Train MAE:  7.378655\n",
      "    Val MSE:    120.448930  |  Val MAE:    3.268572\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [71/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  904.949972  |  Train MAE:  7.377642\n",
      "    Val MSE:    126.110024  |  Val MAE:    3.275062\n",
      "    LR:          0.000500\n",
      "\n",
      "Epoch [72/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  890.468550  |  Train MAE:  7.383769\n",
      "    Val MSE:    174.215897  |  Val MAE:    3.694357\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [73/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  888.276683  |  Train MAE:  7.243060\n",
      "    Val MSE:    180.023762  |  Val MAE:    3.766749\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [74/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  860.151492  |  Train MAE:  7.192071\n",
      "    Val MSE:    112.053099  |  Val MAE:    3.136679\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [75/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  891.257885  |  Train MAE:  7.301981\n",
      "    Val MSE:    112.474065  |  Val MAE:    3.073097\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [76/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  887.264017  |  Train MAE:  7.277056\n",
      "    Val MSE:    108.863956  |  Val MAE:    3.125735\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [77/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  852.978909  |  Train MAE:  7.161894\n",
      "    Val MSE:    122.818532  |  Val MAE:    3.234320\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [78/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  865.125663  |  Train MAE:  7.142685\n",
      "    Val MSE:    105.200524  |  Val MAE:    3.041800\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [79/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  865.068909  |  Train MAE:  7.263745\n",
      "    Val MSE:    220.694430  |  Val MAE:    4.191202\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [80/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  884.672480  |  Train MAE:  7.280771\n",
      "    Val MSE:    205.649985  |  Val MAE:    4.020611\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [81/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  839.051141  |  Train MAE:  7.138034\n",
      "    Val MSE:    129.185816  |  Val MAE:    3.310641\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [82/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  840.501755  |  Train MAE:  7.097909\n",
      "    Val MSE:    97.851975  |  Val MAE:    2.932342\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [83/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  880.146413  |  Train MAE:  7.287435\n",
      "    Val MSE:    155.467660  |  Val MAE:    3.554960\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [84/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  882.058303  |  Train MAE:  7.249299\n",
      "    Val MSE:    110.716579  |  Val MAE:    3.142481\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [85/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  845.865721  |  Train MAE:  7.082998\n",
      "    Val MSE:    115.520706  |  Val MAE:    3.247522\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [86/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  869.855701  |  Train MAE:  7.168902\n",
      "    Val MSE:    98.381899  |  Val MAE:    2.946471\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [87/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  878.968908  |  Train MAE:  7.225112\n",
      "    Val MSE:    125.520439  |  Val MAE:    3.232385\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [88/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  869.657765  |  Train MAE:  7.241040\n",
      "    Val MSE:    102.448059  |  Val MAE:    3.035439\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [89/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  870.111025  |  Train MAE:  7.235285\n",
      "    Val MSE:    108.186259  |  Val MAE:    3.111168\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [90/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  852.326439  |  Train MAE:  7.193149\n",
      "    Val MSE:    111.075901  |  Val MAE:    3.057001\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [91/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  852.302578  |  Train MAE:  7.154642\n",
      "    Val MSE:    119.057822  |  Val MAE:    3.216067\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [92/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  861.555507  |  Train MAE:  7.210837\n",
      "    Val MSE:    123.889543  |  Val MAE:    3.221946\n",
      "    LR:          0.000250\n",
      "\n",
      "Epoch [93/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  848.431808  |  Train MAE:  7.160188\n",
      "    Val MSE:    122.575677  |  Val MAE:    3.249327\n",
      "    LR:          0.000125\n",
      "\n",
      "Epoch [94/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  847.761483  |  Train MAE:  7.138866\n",
      "    Val MSE:    115.396045  |  Val MAE:    3.107068\n",
      "    LR:          0.000125\n",
      "\n",
      "Epoch [95/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  886.480202  |  Train MAE:  7.253784\n",
      "    Val MSE:    117.373538  |  Val MAE:    3.082705\n",
      "    LR:          0.000125\n",
      "\n",
      "Epoch [96/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  828.274369  |  Train MAE:  7.035837\n",
      "    Val MSE:    110.536592  |  Val MAE:    3.031022\n",
      "    LR:          0.000125\n",
      "\n",
      "Epoch [97/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  850.808447  |  Train MAE:  7.128403\n",
      "    Val MSE:    102.696602  |  Val MAE:    2.940809\n",
      "    LR:          0.000125\n",
      "\n",
      "Epoch [98/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  817.694818  |  Train MAE:  6.972840\n",
      "    Val MSE:    107.952716  |  Val MAE:    3.029277\n",
      "    LR:          0.000125\n",
      "\n",
      "Epoch [99/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  832.529526  |  Train MAE:  7.066206\n",
      "    Val MSE:    136.927436  |  Val MAE:    3.407410\n",
      "    LR:          0.000125\n",
      "\n",
      "Epoch [100/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  858.485430  |  Train MAE:  7.150609\n",
      "    Val MSE:    97.851242  |  Val MAE:    2.917159\n",
      "    LR:          0.000125\n",
      "\n",
      "Training completed!\n",
      "Total epochs trained: 100\n",
      "Best validation MSE: 97.851242\n",
      "Best validation MAE: 2.917159\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device, epoch=None, total_epochs=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch_idx, (z, r, mask, t) in enumerate(loader):\n",
    "        z = z.to(device)\n",
    "        r = r.to(device)\n",
    "        mask = mask.to(device)\n",
    "        t = t.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(z, r, mask)\n",
    "        loss = criterion(predictions, t)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        mae = torch.mean(torch.abs(predictions - t)).item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae\n",
    "        n_batches += 1\n",
    "    \n",
    "    if epoch is not None:\n",
    "        print()\n",
    "    \n",
    "    return total_loss / n_batches, total_mae / n_batches\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for z, r, mask, t in loader:\n",
    "            z = z.to(device)\n",
    "            r = r.to(device)\n",
    "            mask = mask.to(device)\n",
    "            t = t.to(device)\n",
    "            \n",
    "            predictions = model(z, r, mask)\n",
    "            loss = criterion(predictions, t)\n",
    "            \n",
    "            mae = torch.mean(torch.abs(predictions - t)).item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae\n",
    "            n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches, total_mae / n_batches\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "train_losses = []\n",
    "train_maes = []\n",
    "val_losses = []\n",
    "val_maes = []\n",
    "best_val_loss = float('inf')\n",
    "best_val_mae = float('inf')\n",
    "patience_counter = 0\n",
    "max_patience = 20\n",
    "current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "print(\"Starting Training - GSCNN\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training phase\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "    print(f\"  Training...\", end=' ')\n",
    "    train_loss, train_mae = train_epoch(model, train_loader, criterion, optimizer, device, epoch+1, n_epochs)\n",
    "    train_losses.append(train_loss)\n",
    "    train_maes.append(train_mae)\n",
    "    \n",
    "    # Validation phase\n",
    "    print(f\"  Validating...\", end=' ')\n",
    "    val_loss, val_mae = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_maes.append(val_mae)\n",
    "    \n",
    "    # Update learning rate\n",
    "    old_lr = current_lr\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    lr_reduced = old_lr != current_lr\n",
    "    \n",
    "    # Early stopping check (using MSE loss)\n",
    "    improved = val_loss < best_val_loss\n",
    "    if improved:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_mae = val_mae\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_gscnn_model.pth')\n",
    "        save_status = \"Saved\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        save_status = \"\"\n",
    "    \n",
    "    print(f\"  Results:\")\n",
    "    print(f\"    Train MSE:  {train_loss:.6f}  |  Train MAE:  {train_mae:.6f}\")\n",
    "    print(f\"    Val MSE:    {val_loss:.6f}  |  Val MAE:    {val_mae:.6f}\")\n",
    "    print(f\"    LR:          {current_lr:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= max_patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Total epochs trained: {len(train_losses)}\")\n",
    "print(f\"Best validation MSE: {best_val_loss:.6f}\")\n",
    "print(f\"Best validation MAE: {best_val_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fdee9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# MAE curves\n",
    "ax.plot(train_maes, label='Train MAE', linewidth=2)\n",
    "ax.plot(val_maes, label='Validation MAE', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.set_title('GSCNN Training Curves - MAE')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41fb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147e7314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the test set\n",
      "Test MSE:  90.831713\n",
      "Test MAE:  2.873033\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the model on the test set\")\n",
    "\n",
    "model.load_state_dict(torch.load('best_gscnn_model.pth'))\n",
    "model.eval()\n",
    "test_loss, test_mae = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test MSE:  {test_loss:.6f}\")\n",
    "print(f\"Test MAE:  {test_mae:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
