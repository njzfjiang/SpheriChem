{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2fad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32740a5b",
   "metadata": {},
   "source": [
    "## Load the dataset and pre process\n",
    "Note: here we use simulated linear positions for GNN (positions are used only for distance-based connectivity), while real 3D molecular coordinates are available for GSCNN (which requires accurate geometry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc02b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "  X (Coulomb matrices): (7211, 23, 23)\n",
      "  T (Properties): (7211, 14)\n",
      "Extracted Z for 7211 molecules\n"
     ]
    }
   ],
   "source": [
    "data = scipy.io.loadmat('../qm7b.mat')\n",
    "\n",
    "# Extract available data\n",
    "X = data['X']  # Coulomb matrices: (7211, 23, 23)\n",
    "T = data['T']  # Properties: (7211, 14)\n",
    "names = data['names']  # Property names: (14,)\n",
    "\n",
    "X = np.array(X)\n",
    "T = np.array(T)\n",
    "\n",
    "# Get property names\n",
    "if names.ndim > 1:\n",
    "    property_names = [str(names[i][0]) for i in range(len(names))]\n",
    "else:\n",
    "    property_names = [str(names[i]) for i in range(len(names))]\n",
    "\n",
    "print(f\"Dataset shapes:\")\n",
    "print(f\"  X (Coulomb matrices): {X.shape}\")\n",
    "print(f\"  T (Properties): {T.shape}\")\n",
    "\n",
    "# Extract atomic numbers from Coulomb matrix diagonals\n",
    "# C_ii = 0.5 * Z_i^2.4, so Z_i = (2 * C_ii)^(1/2.4)\n",
    "Z_list = []\n",
    "for i in range(len(X)):\n",
    "    coulomb_diag = np.diag(X[i])\n",
    "    # Find non-zero diagonal elements (actual atoms)\n",
    "    atom_indices = np.where(coulomb_diag > 1e-6)[0]\n",
    "    if len(atom_indices) > 0:\n",
    "        z_vals = (2 * coulomb_diag[atom_indices]) ** (1.0 / 2.4)\n",
    "        z_vals = np.round(z_vals).astype(int)\n",
    "        Z_list.append(z_vals)\n",
    "    else:\n",
    "        Z_list.append(np.array([1]))\n",
    "\n",
    "\n",
    "R_list = []\n",
    "for i, z_vals in enumerate(Z_list):\n",
    "    n_atoms = len(z_vals)\n",
    "    r_vals = np.zeros((n_atoms, 3))\n",
    "    for j in range(n_atoms):\n",
    "        r_vals[j, 0] = j * 1.5  # Simple spacing along x-axis\n",
    "    R_list.append(r_vals)\n",
    "\n",
    "Z = np.array(Z_list, dtype=object)\n",
    "R = np.array(R_list, dtype=object)\n",
    "\n",
    "print(f\"Extracted Z for {len(Z)} molecules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df52ab2",
   "metadata": {},
   "source": [
    "## GAT Architecture\n",
    "\n",
    "Define GATConv (Graph Attention Network Convolution) and GAT model (embedding → GAT layers → global pooling → property prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f409fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATConv(nn.Module):\n",
    "    \"\"\"Graph Attention Network Convolution Layer\"\"\"\n",
    "    def __init__(self, n_in, n_out, n_heads=1, dropout=0.1):\n",
    "        super(GATConv, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_out // n_heads\n",
    "        \n",
    "        # Linear transformation\n",
    "        self.W = nn.Linear(n_in, n_out)\n",
    "        \n",
    "        # Attention mechanism: compute attention scores\n",
    "        self.attn = nn.Linear(2 * self.head_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x, edge_index, mask):\n",
    "        \"\"\"\n",
    "        GAT convolution: attention-weighted aggregation\n",
    "        \"\"\"\n",
    "        row, col = edge_index\n",
    "        batch_size_n_atoms, n_in = x.shape\n",
    "        \n",
    "        # Linear transformation\n",
    "        h = self.W(x)  # (batch_size_n_atoms, n_out)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        if self.n_heads > 1:\n",
    "            h = h.view(batch_size_n_atoms, self.n_heads, self.head_dim)\n",
    "        else:\n",
    "            h = h.unsqueeze(1)  # (batch_size_n_atoms, 1, head_dim)\n",
    "        \n",
    "        # Compute attention scores for edges\n",
    "        if len(row) > 0:\n",
    "            # Get source and target node features\n",
    "            h_src = h[row]  # (n_edges, n_heads, head_dim)\n",
    "            h_tgt = h[col]  # (n_edges, n_heads, head_dim)\n",
    "            \n",
    "            # Concatenate source and target features\n",
    "            h_cat = torch.cat([h_src, h_tgt], dim=-1)  # (n_edges, n_heads, 2*head_dim)\n",
    "            \n",
    "            # Compute attention scores\n",
    "            attn_scores = self.attn(h_cat).squeeze(-1)  # (n_edges, n_heads)\n",
    "            attn_scores = self.leaky_relu(attn_scores)\n",
    "            \n",
    "            # Apply mask to attention scores (mask out invalid edges)\n",
    "            edge_mask = (mask[row] * mask[col]).unsqueeze(-1)  # (n_edges, 1)\n",
    "            attn_scores = attn_scores * edge_mask + (1 - edge_mask) * (-1e9)\n",
    "            \n",
    "            # Softmax over neighbors for each node\n",
    "            # Process each node separately for stable softmax\n",
    "            out = torch.zeros(batch_size_n_atoms, self.n_heads, self.head_dim, device=x.device)\n",
    "            \n",
    "            # Group edges by source node\n",
    "            for node_idx in range(batch_size_n_atoms):\n",
    "                # Find all edges where this node is the source\n",
    "                node_mask = (row == node_idx)\n",
    "                if not node_mask.any():\n",
    "                    continue\n",
    "                \n",
    "                # Get attention scores for this node's neighbors\n",
    "                node_attn = attn_scores[node_mask]  # (n_neighbors, n_heads)\n",
    "                \n",
    "                # Apply softmax\n",
    "                node_attn_weights = F.softmax(node_attn, dim=0)  # (n_neighbors, n_heads)\n",
    "                node_attn_weights = node_attn_weights.unsqueeze(-1)  # (n_neighbors, n_heads, 1)\n",
    "                \n",
    "                # Get neighbor features\n",
    "                neighbor_indices = col[node_mask]\n",
    "                neighbor_features = h[neighbor_indices]  # (n_neighbors, n_heads, head_dim)\n",
    "                \n",
    "                # Weighted aggregation\n",
    "                weighted = neighbor_features * node_attn_weights  # (n_neighbors, n_heads, head_dim)\n",
    "                out[node_idx] = weighted.sum(dim=0)  # (n_heads, head_dim)\n",
    "        else:\n",
    "            out = h\n",
    "        \n",
    "        # Reshape back\n",
    "        if self.n_heads > 1:\n",
    "            out = out.view(batch_size_n_atoms, -1)  # (batch_size_n_atoms, n_out)\n",
    "        else:\n",
    "            out = out.squeeze(1)  # (batch_size_n_atoms, n_out)\n",
    "        \n",
    "        # Apply activation and dropout\n",
    "        out = F.elu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Apply mask\n",
    "        out = out * mask.unsqueeze(-1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"Simplified GAT model for molecular property prediction\"\"\"\n",
    "    def __init__(self, n_atom_basis=128, n_layers=2, n_heads=1,\n",
    "                 n_out=14, max_atoms=23, dropout=0.1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.n_atom_basis = n_atom_basis\n",
    "        self.n_layers = n_layers\n",
    "        self.max_atoms = max_atoms\n",
    "        \n",
    "        # Embedding layer for atomic numbers\n",
    "        self.embedding = nn.Embedding(100, n_atom_basis)\n",
    "        \n",
    "        # GAT layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            self.gat_layers.append(\n",
    "                GATConv(n_atom_basis, n_atom_basis, n_heads=n_heads, dropout=dropout)\n",
    "            )\n",
    "        \n",
    "        # Simplified property prediction head\n",
    "        self.property_head = nn.Sequential(\n",
    "            nn.Linear(n_atom_basis, n_atom_basis // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_atom_basis // 2, n_out)\n",
    "        )\n",
    "        \n",
    "    def build_edge_index(self, positions, mask, cutoff=5.0):\n",
    "        batch_size, n_atoms, _ = positions.shape\n",
    "        device = positions.device\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        pos_i = positions.unsqueeze(2)\n",
    "        pos_j = positions.unsqueeze(1)\n",
    "        diff = pos_i - pos_j\n",
    "        distances = torch.norm(diff, dim=-1)\n",
    "        \n",
    "        mask_i = mask.unsqueeze(2)\n",
    "        mask_j = mask.unsqueeze(1)\n",
    "        neighbor_mask = (mask_i * mask_j).float()\n",
    "        \n",
    "        eye = torch.eye(n_atoms, device=device).unsqueeze(0)\n",
    "        neighbor_mask = neighbor_mask * (1 - eye)\n",
    "        \n",
    "        edge_mask = (distances < cutoff) * neighbor_mask\n",
    "        \n",
    "        # Build edge index for each batch\n",
    "        edge_indices = []\n",
    "        for b in range(batch_size):\n",
    "            edges = torch.nonzero(edge_mask[b], as_tuple=False)\n",
    "            if len(edges) > 0:\n",
    "                edges_offset = edges + b * n_atoms\n",
    "                edge_indices.append(edges_offset.t())\n",
    "            else:\n",
    "                self_loop = torch.arange(n_atoms, device=device) + b * n_atoms\n",
    "                edge_indices.append(torch.stack([self_loop, self_loop]))\n",
    "        \n",
    "        if len(edge_indices) > 0:\n",
    "            edge_index = torch.cat(edge_indices, dim=1)\n",
    "        else:\n",
    "            # Fallback: create self-loops\n",
    "            edge_index = torch.stack([\n",
    "                torch.arange(batch_size * n_atoms, device=device),\n",
    "                torch.arange(batch_size * n_atoms, device=device)\n",
    "            ])\n",
    "        \n",
    "        return edge_index\n",
    "    \n",
    "    def forward(self, atomic_numbers, positions, mask):\n",
    "        batch_size, n_atoms, _ = positions.shape\n",
    "        x = self.embedding(atomic_numbers)\n",
    "        \n",
    "        edge_index = self.build_edge_index(positions, mask)\n",
    "        x_flat = x.view(-1, self.n_atom_basis)\n",
    "        mask_flat = mask.view(-1)\n",
    "        \n",
    "        # Apply GAT layers\n",
    "        for gat_layer in self.gat_layers:\n",
    "            x_flat = gat_layer(x_flat, edge_index, mask_flat)\n",
    "        \n",
    "        # Global pooling: mean over atoms\n",
    "        x = x_flat.view(batch_size, n_atoms, self.n_atom_basis)\n",
    "        mask_expanded = mask.unsqueeze(-1).float()\n",
    "        x_masked = x * mask_expanded\n",
    "        x_sum = x_masked.sum(dim=1)\n",
    "        n_atoms = mask.sum(dim=1, keepdim=True).float()\n",
    "        x_mean = x_sum / (n_atoms + 1e-8)\n",
    "        \n",
    "        # Direct property prediction\n",
    "        properties = self.property_head(x_mean)\n",
    "        \n",
    "        return properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47ae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QM7bDataset(Dataset):\n",
    "    def __init__(self, Z, R, T, max_atoms=23):\n",
    "        self.Z = Z\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.n_molecules = len(Z)\n",
    "        self.max_atoms = max_atoms\n",
    "        self.n_properties = T.shape[1] if T.ndim > 1 else 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_molecules\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        z = torch.from_numpy(self.Z[idx]).long()\n",
    "        r = torch.from_numpy(self.R[idx]).float()\n",
    "        t = torch.from_numpy(self.T[idx]).float() if self.T.ndim > 1 else torch.tensor([self.T[idx]]).float()\n",
    "        \n",
    "        mask = (z > 0).float()\n",
    "        \n",
    "        return z, r, mask, t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0420d6",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "Pad/truncate molecules to fixed size (23 atoms max) for batch processing. Handle variable-length molecular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ac868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_molecular_data(Z, R, max_atoms=23):\n",
    "    n_molecules = len(Z)\n",
    "    \n",
    "    Z_processed = []\n",
    "    R_processed = []\n",
    "    \n",
    "    for i in range(n_molecules):\n",
    "        z_vals = Z[i] if isinstance(Z[i], np.ndarray) else np.array([Z[i]])\n",
    "        z_vals = z_vals.flatten()\n",
    "        \n",
    "        r_vals = R[i] if isinstance(R[i], np.ndarray) else np.zeros((len(z_vals), 3))\n",
    "        if r_vals.ndim == 1:\n",
    "            if len(r_vals) >= len(z_vals) * 3:\n",
    "                r_vals = r_vals[:len(z_vals)*3].reshape(-1, 3)\n",
    "            else:\n",
    "                r_vals = np.zeros((len(z_vals), 3))\n",
    "        elif r_vals.ndim == 2 and r_vals.shape[1] != 3:\n",
    "            r_vals = np.zeros((len(z_vals), 3))\n",
    "        \n",
    "        if r_vals.shape[0] < len(z_vals):\n",
    "            r_padded_temp = np.zeros((len(z_vals), 3))\n",
    "            r_padded_temp[:r_vals.shape[0]] = r_vals\n",
    "            r_vals = r_padded_temp\n",
    "        elif r_vals.shape[0] > len(z_vals):\n",
    "            r_vals = r_vals[:len(z_vals)]\n",
    "        \n",
    "        n_atoms = min(len(z_vals), max_atoms)\n",
    "        z_padded = np.zeros(max_atoms, dtype=np.int64)\n",
    "        r_padded = np.zeros((max_atoms, 3), dtype=np.float32)\n",
    "        \n",
    "        z_padded[:n_atoms] = z_vals[:n_atoms]\n",
    "        r_padded[:n_atoms] = r_vals[:n_atoms]\n",
    "        \n",
    "        Z_processed.append(z_padded)\n",
    "        R_processed.append(r_padded)\n",
    "    \n",
    "    return np.array(Z_processed), np.array(R_processed)\n",
    "\n",
    "Z_processed, R_processed = prepare_molecular_data(Z, R, max_atoms=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805ee8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5768\n",
      "Validation samples: 721\n",
      "Test samples: 722\n"
     ]
    }
   ],
   "source": [
    "# Split data to 80% train, 10% validation, 10% test\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(T)), test_size=0.2, random_state=42\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QM7bDataset(\n",
    "    Z_processed[train_idx], \n",
    "    R_processed[train_idx], \n",
    "    T[train_idx],\n",
    "    max_atoms=23\n",
    ")\n",
    "val_dataset = QM7bDataset(\n",
    "    Z_processed[val_idx], \n",
    "    R_processed[val_idx], \n",
    "    T[val_idx],\n",
    "    max_atoms=23\n",
    ")\n",
    "test_dataset = QM7bDataset(\n",
    "    Z_processed[test_idx], \n",
    "    R_processed[test_idx], \n",
    "    T[test_idx],\n",
    "    max_atoms=23\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99883be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 55,504\n"
     ]
    }
   ],
   "source": [
    "model = GAT(\n",
    "    n_atom_basis=128,\n",
    "    n_layers=2,\n",
    "    n_heads=1,  # Single head for simplicity\n",
    "    n_out=T.shape[1],  # Predict all 14 properties\n",
    "    max_atoms=23,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch [1/100]\n",
      "  Training... \n",
      "  Validating...   Results:\n",
      "    Train MSE:  61456.887494  |  Train MAE:  64.116985\n",
      "    Val MSE:    4486.390402  |  Val MAE:    18.152570\n",
      "    LR:          0.001000\n",
      "\n",
      "Epoch [2/100]\n",
      "  Training... "
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device, epoch=None, total_epochs=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch_idx, (z, r, mask, t) in enumerate(loader):\n",
    "        z = z.to(device)\n",
    "        r = r.to(device)\n",
    "        mask = mask.to(device)\n",
    "        t = t.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(z, r, mask)\n",
    "        loss = criterion(predictions, t)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate MAE for this batch\n",
    "        mae = torch.mean(torch.abs(predictions - t)).item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae\n",
    "        n_batches += 1\n",
    "    \n",
    "    if epoch is not None:\n",
    "        print()\n",
    "    \n",
    "    return total_loss / n_batches, total_mae / n_batches\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for z, r, mask, t in loader:\n",
    "            z = z.to(device)\n",
    "            r = r.to(device)\n",
    "            mask = mask.to(device)\n",
    "            t = t.to(device)\n",
    "            \n",
    "            predictions = model(z, r, mask)\n",
    "            loss = criterion(predictions, t)\n",
    "            \n",
    "            # Calculate MAE for this batch\n",
    "            mae = torch.mean(torch.abs(predictions - t)).item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae\n",
    "            n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches, total_mae / n_batches\n",
    "\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "train_losses = []\n",
    "train_maes = []\n",
    "val_losses = []\n",
    "val_maes = []\n",
    "best_val_loss = float('inf')\n",
    "best_val_mae = float('inf')\n",
    "patience_counter = 0\n",
    "max_patience = 20\n",
    "current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "print(\"Starting Training\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training phase\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "    print(f\"  Training...\", end=' ')\n",
    "    train_loss, train_mae = train_epoch(model, train_loader, criterion, optimizer, device, epoch+1, n_epochs)\n",
    "    train_losses.append(train_loss)\n",
    "    train_maes.append(train_mae)\n",
    "    \n",
    "    # Validation phase\n",
    "    print(f\"  Validating...\", end=' ')\n",
    "    val_loss, val_mae = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_maes.append(val_mae)\n",
    "    \n",
    "    # Update learning rate\n",
    "    old_lr = current_lr\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    lr_reduced = old_lr != current_lr\n",
    "    \n",
    "    # Early stopping check (using MSE loss)\n",
    "    improved = val_loss < best_val_loss\n",
    "    if improved:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_mae = val_mae\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), '../best_gat_model.pth')\n",
    "        save_status = \"Saved\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        save_status = \"\"\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"  Results:\")\n",
    "    print(f\"    Train MSE:  {train_loss:.6f}  |  Train MAE:  {train_mae:.6f}\")\n",
    "    print(f\"    Val MSE:    {val_loss:.6f}  |  Val MAE:    {val_mae:.6f}\")\n",
    "    print(f\"    LR:          {current_lr:.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= max_patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"Training completed\")\n",
    "print(f\"Total epochs trained: {len(train_losses)}\")\n",
    "print(f\"Best validation MSE: {best_val_loss:.6f}\")\n",
    "print(f\"Best validation MAE: {best_val_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# MAE curves\n",
    "ax.plot(train_maes, label='Train MAE', linewidth=2)\n",
    "ax.plot(val_maes, label='Validation MAE', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.set_title('GAT Training Curves - MAE')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db917419",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the model on the test set\")\n",
    "\n",
    "model.load_state_dict(torch.load('../best_gat_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test MSE:  {test_loss:.6f}\")\n",
    "print(f\"Test MAE:  {test_mae:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
